{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72cbc17e",
   "metadata": {},
   "source": [
    "## Step 3: Simulated Annealing for Interlayer Dependency\n",
    "\n",
    "Run simulated annealing to run joint search over multiple layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6df1faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import yaml\n",
    "import shutil\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ff6a34",
   "metadata": {},
   "source": [
    "First, we have to define the architecture and the workload similar as before.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f08da56",
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration_dict = {}\n",
    "\n",
    "# template design (with constraints and memory hierarchy representing \"dataflow\")\n",
    "configuration_dict['TEMPLATE_DESIGN'] = 'eyeriss_like'\n",
    "\n",
    "# number of bits used for I/O/W; we assume integer\n",
    "configuration_dict['WORDBITS'] = 16\n",
    "\n",
    "# DRAM bandwidth setting: words / cycle (not bits / cycle)\n",
    "configuration_dict['DRAM_READ_BANDWIDTH'] = 32\n",
    "configuration_dict['DRAM_WRITE_BANDWIDTH'] = 32\n",
    "\n",
    "# SRAM setting\n",
    "# - do we have a single shared glb or multiple glbs for each datatype? \n",
    "# - for each glb (if shared, just one), define depth/width/#banks and bandwidths\n",
    "configuration_dict['SRAM_SHARED'] = True\n",
    "configuration_dict['SRAM_DEPTH'] = [2 ** 13]\n",
    "configuration_dict['SRAM_WIDTH'] = [2 ** 7]\n",
    "configuration_dict['SRAM_BANKS'] = [32]                     # SRAM width and SRAM banks define the maximum possible bandwidth\n",
    "configuration_dict['SRAM_READ_BANDWIDTH'] = [32]\n",
    "configuration_dict['SRAM_WRITE_BANDWIDTH'] = [32]\n",
    "\n",
    "# PE array setting\n",
    "# - shape of PE array X x Y\n",
    "# - whether a PE has a shared scratchpad or separate scratchpads for each datatype\n",
    "configuration_dict['PE_X'] = 14\n",
    "configuration_dict['PE_Y'] = 12\n",
    "configuration_dict['PE_SPAD_SHARED'] = False\n",
    "configuration_dict['PE_SPAD_DEPTH'] = [192, 12, 16]         # Weight, IFmap, OFmap\n",
    "configuration_dict['PE_SPAD_WIDTH'] = [16, 16, 16]\n",
    "\n",
    "# Cryptographic engine setting\n",
    "# - type of cryptographic engine + dram (LPDDR4 + AES-GCM)\n",
    "# - cycle for AES-GCM \n",
    "# - whether the cryptographic engines are shared among all datatypes or assigned to each datatype\n",
    "configuration_dict['CRYPT_ENGINE_TYPE'] = 'effective_lpddr4_aesgcm'\n",
    "configuration_dict['CRYPT_ENGINE_CYCLE_PER_BLOCK'] = 11            # avg. cycle/128bit\n",
    "\n",
    "configuration_dict['CRYPT_ENGINE_SHARED'] = False\n",
    "configuration_dict['CRYPT_ENGINE_COUNT'] = [1, 1, 1]\n",
    "\n",
    "configuration_dict['EFFECTIVE_CONSERVATIVE'] = True\n",
    "\n",
    "# Create directory for this configuration if it doesn't exist already\n",
    "# iterate through design folders to check if any pre-exisiting folder\n",
    "design_dir = 'designs/{}'.format(configuration_dict['TEMPLATE_DESIGN'])\n",
    "arch_dir = None\n",
    "total_vers = 0\n",
    "for path in os.listdir(design_dir):\n",
    "    if path != 'template' and os.path.isdir(os.path.join(design_dir, path)):\n",
    "        try:\n",
    "            with open(os.path.join(design_dir, path, 'config.yaml'), 'r') as f:\n",
    "                config_file = yaml.safe_load(f)\n",
    "            total_vers += 1\n",
    "            if config_file == configuration_dict:\n",
    "                arch_dir = path\n",
    "                print(\"Pre-existing folder found. Setting the arch_dir to {}\".format(arch_dir))\n",
    "                break\n",
    "        except:\n",
    "            print(\"No config.yaml file in the directory {}\".format(str(os.path.join(design_dir, path))))\n",
    "            \n",
    "if arch_dir == None:\n",
    "    raise NameError(\"Architecture is not found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5d87d1",
   "metadata": {},
   "source": [
    "..else if you know which folder you want to use, specify here instead of running the above cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98278c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "design_dir = 'designs/{}'.format('eyeriss_like') # define your design name here\n",
    "\n",
    "arch_ver = 0\n",
    "arch_dir = 'ver{}'.format(arch_ver)              # sub directory under designs/{name}/{arch_dir}\n",
    "with open(os.path.join(design_dir, arch_dir, 'config.yaml'), 'r') as f:\n",
    "    configuration_dict = yaml.safe_load(f)\n",
    "print(\"Setting the architecture directory to: {}\".format(os.path.join(design_dir, arch_dir)))\n",
    "print(\"Printing configuration:\")\n",
    "for key, value in configuration_dict.items():\n",
    "    print(\"{}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46cb12a",
   "metadata": {},
   "source": [
    "Define the workload here. Skip the pytorch2timeloop conversion (should be done when generating loopnests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b904e7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision.models as model_zoo\n",
    "\n",
    "import pytorch2timeloop as pytorch2timeloop\n",
    "\n",
    "# Note: this version only supports nn.Conv2d (both normal convs and depthwise/pointwise convs) and nn.Linear\n",
    "\n",
    "# AlexNet\n",
    "# model_name = 'alexnet'\n",
    "# net = model_zoo.alexnet(pretrained=False)\n",
    "\n",
    "# ResNet18\n",
    "# model_name = 'resnet18'\n",
    "# net = model_zoo.resnet18(pretrained=False)\n",
    "\n",
    "# MobilenetV2\n",
    "model_name = 'mobilenet_v2'\n",
    "net = model_zoo.mobilenet_v2(pretrained=False)\n",
    "\n",
    "# Input / Batch info\n",
    "input_size = (3, 224, 224)\n",
    "batch_size = 1\n",
    "\n",
    "print(net)\n",
    "\n",
    "top_dir = 'workloads'\n",
    "sub_dir = '{}_batch{}'.format(model_name, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993f4bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = Path(os.getcwd())\n",
    "timeloop_dir = 'designs/{}/{}'.format(configuration_dict['TEMPLATE_DESIGN'], arch_dir)\n",
    "\n",
    "n_layers = 0\n",
    "layer_dict = {}\n",
    "layer_duplicate_info = {}\n",
    "unique_layers = []\n",
    "for module in net.modules():\n",
    "    if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n",
    "        n_layers += 1\n",
    "        if n_layers not in layer_dict.keys():\n",
    "            workload_path = os.path.join(base_dir, top_dir, sub_dir, '{}_layer{}.yaml'.format(sub_dir, n_layers))\n",
    "            with open(workload_path, 'r') as f:\n",
    "                workload_info = yaml.safe_load(f)\n",
    "            layer_dict[n_layers] = workload_info\n",
    "        \n",
    "        # identify the earliest duplicate layer\n",
    "        for key in range(1, n_layers):\n",
    "            if layer_dict[key] == layer_dict[n_layers]:\n",
    "                layer_duplicate_info[n_layers] = key\n",
    "                break\n",
    "        if n_layers not in layer_duplicate_info:\n",
    "            unique_layers.append(n_layers)\n",
    "            \n",
    "print(layer_duplicate_info)\n",
    "print(unique_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21081b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_layer_dependency_utils import BackpropGraph\n",
    "\n",
    "workload_path = os.path.join(base_dir, top_dir, sub_dir, 'layer_info_interlayer.yaml')\n",
    "\n",
    "try:\n",
    "    with open(workload_path, 'r') as f:\n",
    "        layer_info = yaml.safe_load(f)\n",
    "    for layer_idx in range(1, n_layers + 1):\n",
    "        print(layer_idx, layer_info[layer_idx])\n",
    "except:\n",
    "\n",
    "    graph = BackpropGraph(net, [1, input_size[0], input_size[1], input_size[2]])\n",
    "    consecutive_dict, dependent_dict = graph.get_dependency_info()\n",
    "\n",
    "    # construct layer_info\n",
    "    layer_info = {}\n",
    "    for layer_idx in range(1, n_layers + 1):\n",
    "        info = {}\n",
    "        if layer_idx in unique_layers:\n",
    "            info['layer_id_for_timeloop'] = layer_idx\n",
    "        else:\n",
    "            info['layer_id_for_timeloop'] = layer_duplicate_info[layer_idx]\n",
    "        info['prev_layer'] = []\n",
    "        info['next_layer'] = []\n",
    "        info['dependent_prev_layer'] = []\n",
    "        info['dependent_next_layer'] = []\n",
    "        layer_info[layer_idx] = info\n",
    "\n",
    "    for layer_idx in range(1, n_layers + 1):\n",
    "        consecutive = consecutive_dict[layer_idx]\n",
    "        dependent = dependent_dict[layer_idx]\n",
    "        layer_info[layer_idx]['next_layer'].extend(consecutive)\n",
    "        for i in consecutive:\n",
    "            layer_info[i]['prev_layer'].append(layer_idx)\n",
    "        if len(dependent) > 0 and not ignore_interlayer:\n",
    "            layer_info[layer_idx]['dependent_next_layer'].extend(dependent)   \n",
    "            for i in dependent:\n",
    "                layer_info[i]['dependent_prev_layer'].append(layer_idx)\n",
    "\n",
    "    for layer_idx in range(1, n_layers + 1):\n",
    "        print(layer_idx, layer_info[layer_idx])\n",
    "\n",
    "    # store therresults - this can take long for deep models like MobileNet..\n",
    "    # f\"{base_dir/top_dir/sub_dir/sub_dir}_layer{layer_id}.yaml \"\n",
    "    with open(workload_path, 'w') as f:\n",
    "        _ = yaml.dump(layer_info, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b08e62",
   "metadata": {},
   "source": [
    "### Prepare the folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370e0c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(os.path.join(base_dir, timeloop_dir, 'joint_topk')):\n",
    "    os.mkdir(os.path.join(base_dir, timeloop_dir, 'joint_topk'))\n",
    "if not os.path.exists(os.path.join(base_dir, timeloop_dir, 'joint_topk', sub_dir)):\n",
    "    os.mkdir(os.path.join(base_dir, timeloop_dir, 'joint_topk', sub_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de831167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define top-k you used for timeloop-topk\n",
    "topk = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50723122",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Copy necessary files to the joint_topk folder\n",
    "from authblock_assignment import AuthBlockAssignment\n",
    "\n",
    "base_cost_dict, base_rehash_cost_dict, base_block_info_dict = AuthBlockAssignment(n_layers, layer_info, \\\n",
    "                                                                                  base_dir, timeloop_dir, top_dir, sub_dir, \\\n",
    "                                                                                  configuration_dict, \\\n",
    "                                                                                  mode=\"search\", \\\n",
    "                                                                                  joint=False, return_cost_dict=True)\n",
    "\n",
    "baseline_energy = 0\n",
    "baseline_latency = 0\n",
    "baseline_add_mem_traffic = 0\n",
    "\n",
    "for key in base_cost_dict:\n",
    "    baseline_energy += base_cost_dict[key]['total_energy'] / 10**6\n",
    "    baseline_latency += base_cost_dict[key]['total_latency']\n",
    "    baseline_add_mem_traffic += base_cost_dict[key]['add_memory_traffic']\n",
    "for key in base_rehash_cost_dict:\n",
    "    baseline_energy += base_rehash_cost_dict[key]['total_energy'] / 10**6\n",
    "    baseline_latency += base_rehash_cost_dict[key]['total_latency']\n",
    "    baseline_add_mem_traffic += base_rehash_cost_dict[key]['add_memory_traffic']   \n",
    "    \n",
    "for layer_idx in range(1, n_layers + 1):\n",
    "    work_dir = os.path.join(base_dir, timeloop_dir, 'joint_topk', sub_dir, 'layer{}'.format(layer_idx))\n",
    "    if not os.path.exists(work_dir):\n",
    "        os.mkdir(work_dir)\n",
    "        \n",
    "    # \"\"\"\n",
    "    for k in range(1, topk + 1):\n",
    "        if not os.path.exists(os.path.join(work_dir, 'eval{}'.format(k))):\n",
    "            os.mkdir(os.path.join(work_dir, 'eval{}'.format(k)))\n",
    "        layer_id_for_timeloop = layer_info[layer_idx]['layer_id_for_timeloop']\n",
    "        cwd = f\"{base_dir/timeloop_dir/'joint_topk'/sub_dir/f'layer{layer_idx}'/f'eval{k}'}\"\n",
    "        if 'M' in workload_info['problem']['instance']:\n",
    "            constraint_pth = base_dir/timeloop_dir/'constraints/*.yaml'\n",
    "        else:\n",
    "            # depthwise\n",
    "            constraint_pth = base_dir/timeloop_dir/'constraints_dw/*.yaml'\n",
    "\n",
    "        timeloopcmd = f\"timeloop-model \" \\\n",
    "              f\"{base_dir/timeloop_dir/'arch/baseline.yaml'} \" \\\n",
    "              f\"{base_dir/timeloop_dir/'arch/components/*.yaml'} \" \\\n",
    "              f\"{base_dir/timeloop_dir/'scheduling'/sub_dir/f'layer{layer_id_for_timeloop}'/f'mapping{k}.yaml'} \" \\\n",
    "              f\"{base_dir/top_dir/sub_dir/sub_dir}_layer{layer_idx}.yaml \"\n",
    "        \n",
    "        try:\n",
    "            os.chdir(cwd)\n",
    "        except:\n",
    "            os.mkdir(cwd)\n",
    "            os.chdir(cwd)\n",
    "        os.system(timeloopcmd)\n",
    "        os.chdir(base_dir)\n",
    "    # \"\"\"\n",
    "\n",
    "    # copy mapping1's result into here\n",
    "    shutil.copy(os.path.join(work_dir, 'eval1', 'timeloop-model.map+stats.xml'), work_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359085b9",
   "metadata": {},
   "source": [
    "### Run simulated annealing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0870f7c",
   "metadata": {},
   "source": [
    "Define the hyperparameters here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59640183",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_temp = 100\n",
    "final_temp = 0.1\n",
    "n_iters = 1000\n",
    "\n",
    "cooling_scheduler = 'linear'\n",
    "\n",
    "# Define layer idx if you don't want to search them for simulated anneling \n",
    "# (e.g., non-conv layers in AlexNet)\n",
    "layers_exclude_from_search = []\n",
    "\n",
    "# TODO: this option should not be used for ResNet18 - bug with dependent layer partial update due to residuals\n",
    "use_partial_update = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed3de35",
   "metadata": {},
   "source": [
    "Run simulated annealing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd76790",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import csv\n",
    "import copy\n",
    "import math\n",
    "\n",
    "from authblock_assignment import PartialUpdateAuthBlockAssignment\n",
    "\n",
    "csv_header = ['Iter', 'Temp', \\\n",
    "              'Cost (J x cycles)', 'Total Latency (cycles)', 'Total Energy (uJ)', 'Additional Off-chip Traffic (bits)']\n",
    "logs = []\n",
    "\n",
    "solution_cost_dict = copy.deepcopy(base_cost_dict)\n",
    "solution_rehash_cost_dict = copy.deepcopy(base_rehash_cost_dict)\n",
    "solution_block_info_dict = copy.deepcopy(base_block_info_dict)\n",
    "\n",
    "current_cost_dict = copy.deepcopy(base_cost_dict)\n",
    "current_rehash_cost_dict = copy.deepcopy(base_rehash_cost_dict)\n",
    "current_block_info_dict = copy.deepcopy(base_block_info_dict)\n",
    "\n",
    "solution_state = [1] * n_layers\n",
    "current_state = [1] * n_layers\n",
    "best_state = [1] * n_layers\n",
    "\n",
    "i = 0\n",
    "cost_best = baseline_energy * baseline_latency\n",
    "\n",
    "layers_for_search = []\n",
    "for idx in range(1, n_layers + 1):\n",
    "    if len(layer_info[idx]['dependent_next_layer']) > 0 or len(layer_info[idx]['dependent_prev_layer']) > 0:\n",
    "        if idx not in layers_exclude_from_search:\n",
    "            layers_for_search.append(idx)\n",
    "            \n",
    "start_time = time.time()\n",
    "while i < n_iters + 1:\n",
    "    # temperature\n",
    "    if cooling_scheduler == 'linear':\n",
    "        current_temp = final_temp + (initial_temp - final_temp) / float(n_iters) * float(n_iters - i)\n",
    "    elif cooling_scheduler == 'cosine':\n",
    "        current_temp = final_temp + 0.5 * (initial_temp - final_temp) * (1 + math.cos(float(i) * math.pi / float(n_iters)))\n",
    "    elif cooling_scheduler == 'quadratic':\n",
    "        current_temp = final_temp + (initial_temp - final_temp) * (float(n_iters - i) / float(n_iters))**2\n",
    "    \n",
    "    layer2change = random.choice(layers_for_search)\n",
    "    neighbor_loopnest = random.choice(list(range(1, topk + 1)))\n",
    "    \n",
    "    current_state[layer2change - 1] = neighbor_loopnest\n",
    "    stats_file = os.path.join(base_dir, timeloop_dir, 'joint_topk', sub_dir, \"layer{}\".format(layer2change), \\\n",
    "                              \"eval{}\".format(neighbor_loopnest), \"timeloop-model.stats.txt\")\n",
    "    with open(stats_file, 'r') as f:\n",
    "        lines = f.read().split('\\n')[-200:]\n",
    "        for line in lines:\n",
    "            if line.startswith('Energy'):\n",
    "                energy = eval(line.split(': ')[1].split(' ')[0]) * float(10**6) # micro to pico\n",
    "                # print(energy)\n",
    "            elif line.startswith('Cycles'):\n",
    "                cycle = eval(line.split(': ')[1])\n",
    "    current_cost_dict[layer2change]['timeloop_energy'] = energy\n",
    "    current_cost_dict[layer2change]['timeloop_cycle'] = cycle\n",
    "    \n",
    "    xml_file = os.path.join(base_dir, timeloop_dir, 'joint_topk', sub_dir, \"layer{}\".format(layer2change), \\\n",
    "                            \"eval{}\".format(neighbor_loopnest), \"timeloop-model.map+stats.xml\")\n",
    "    shutil.copy(xml_file, os.path.join(base_dir, timeloop_dir, 'joint_topk', sub_dir, 'layer{}'.format(layer2change)))\n",
    "    \n",
    "    if use_partial_update:\n",
    "        subset_layers = [layer2change]\n",
    "        subset_layers.extend(layer_info[layer2change]['prev_layer'])\n",
    "        subset_layers.extend(layer_info[layer2change]['next_layer'])\n",
    "        \n",
    "        current_cost_dict, current_rehash_cost_dict, current_block_info_dict = \\\n",
    "        PartialUpdateAuthBlockAssignment(n_layers, layer_info, \\\n",
    "                                         base_dir, timeloop_dir, top_dir, sub_dir, \\\n",
    "                                         configuration_dict, mode=\"search\", \\\n",
    "                                         prev_block_info_dict=current_block_info_dict, subset_layers=subset_layers, \\\n",
    "                                         prev_cost_dict=current_cost_dict, prev_rehash_cost_dict=current_rehash_cost_dict)\n",
    "        \n",
    "    else:\n",
    "        current_cost_dict, current_rehash_cost_dict, current_block_info_dict = \\\n",
    "        PartialUpdateAuthBlockAssignment(n_layers, layer_info, \\\n",
    "                                         base_dir, timeloop_dir, top_dir, sub_dir, \\\n",
    "                                         configuration_dict, \\\n",
    "                                         mode=\"search\", \\\n",
    "                                         prev_block_info_dict=None, subset_layers=[], \\\n",
    "                                         prev_cost_dict=current_cost_dict, prev_rehash_cost_dict=None)\n",
    "        \n",
    "    solution_energy, solution_latency, solution_add_mem_traffic = 0, 0, 0\n",
    "    for key in solution_cost_dict:\n",
    "        solution_energy += solution_cost_dict[key]['total_energy'] / 10**6\n",
    "        solution_latency += solution_cost_dict[key]['total_latency']\n",
    "        solution_add_mem_traffic += solution_cost_dict[key]['add_memory_traffic']\n",
    "    for key in solution_rehash_cost_dict:\n",
    "        solution_energy += solution_rehash_cost_dict[key]['total_energy'] / 10**6\n",
    "        solution_latency += solution_rehash_cost_dict[key]['total_latency']\n",
    "        solution_add_mem_traffic += solution_rehash_cost_dict[key]['add_memory_traffic']\n",
    "    \n",
    "    current_energy, current_latency, current_add_mem_traffic = 0, 0, 0\n",
    "    for key in current_cost_dict:\n",
    "        current_energy += current_cost_dict[key]['total_energy'] / 10**6\n",
    "        current_latency += current_cost_dict[key]['total_latency']\n",
    "        current_add_mem_traffic += current_cost_dict[key]['add_memory_traffic']\n",
    "    for key in current_rehash_cost_dict:\n",
    "        current_energy += current_rehash_cost_dict[key]['total_energy'] / 10**6\n",
    "        current_latency += current_rehash_cost_dict[key]['total_latency']\n",
    "        current_add_mem_traffic += current_rehash_cost_dict[key]['add_memory_traffic']\n",
    "    \n",
    "    cost_solution = solution_energy * solution_latency\n",
    "    cost_current = current_energy * current_latency\n",
    "    cost_diff = (cost_solution - cost_current) / (10 ** 6 * n_layers)\n",
    "    \n",
    "    if cost_current < cost_best:\n",
    "        best_state = copy.deepcopy(current_state)\n",
    "        cost_best = cost_current\n",
    "        print(\"Found best so far: \", best_state, \" .. updating cost_best: {}\".format(cost_best))\n",
    "        \n",
    "        # for i in range(1, n_layers + 1):\n",
    "        #     print(current_cost_dict[i]['total_energy'])\n",
    "        # print(\"-----------------------------------------------\")\n",
    "        # for key in current_rehash_cost_dict.keys():\n",
    "        #     print(current_rehash_cost_dict[key]['total_energy'])\n",
    "        #    \n",
    "        # print(current_latency, current_energy)\n",
    "        # break\n",
    "        \n",
    "    if cost_diff > 0 or (random.uniform(0, 1) < math.exp(cost_diff / current_temp)):\n",
    "        solution_state = copy.deepcopy(current_state)\n",
    "        solution_cost_dict = copy.deepcopy(current_cost_dict)\n",
    "        solution_rehash_cost_dict = copy.deepcopy(current_rehash_cost_dict)\n",
    "        solution_block_info_dict = copy.deepcopy(current_block_info_dict)\n",
    "    else:\n",
    "        # roll-back to the solution state\n",
    "        xml_file = os.path.join(base_dir, timeloop_dir, 'joint_topk', sub_dir, \"layer{}\".format(layer2change), \\\n",
    "                                  \"eval{}\".format(solution_state[layer2change - 1]), \"timeloop-model.map+stats.xml\")\n",
    "        shutil.copy(xml_file, os.path.join(base_dir, timeloop_dir, 'joint_topk', sub_dir, 'layer{}'.format(layer2change)))\n",
    "        current_state = copy.deepcopy(solution_state)\n",
    "        current_cost_dict = copy.deepcopy(solution_cost_dict)\n",
    "        current_rehash_cost_dict = copy.deepcopy(solution_rehash_cost_dict)\n",
    "        current_block_info_dict = copy.deepcopy(solution_block_info_dict)\n",
    "    \n",
    "    solution_energy, solution_latency, solution_add_mem_traffic = 0, 0, 0\n",
    "    for key in solution_cost_dict:\n",
    "        solution_energy += solution_cost_dict[key]['total_energy'] / 10**6\n",
    "        solution_latency += solution_cost_dict[key]['total_latency']\n",
    "        solution_add_mem_traffic += solution_cost_dict[key]['add_memory_traffic']\n",
    "    for key in solution_rehash_cost_dict:\n",
    "        solution_energy += solution_rehash_cost_dict[key]['total_energy'] / 10**6\n",
    "        solution_latency += solution_rehash_cost_dict[key]['total_latency']\n",
    "        solution_add_mem_traffic += solution_rehash_cost_dict[key]['add_memory_traffic']\n",
    "        \n",
    "    # print(\"Solution state: \", solution_state)\n",
    "    print(\"Current iteration: {} (temperature: {:.2f}) -- Latency: {} ({:.2f}% faster), Energy: {} uW ({:.2f}% lower), Add Mem Traffic: {} bits ({:.2f}% smaller)\"\\\n",
    "          .format(i+1, current_temp, solution_latency, (baseline_latency - solution_latency) / float(baseline_latency) * 100. , \\\n",
    "                  solution_energy, (baseline_energy - solution_energy) / baseline_energy * 100., \\\n",
    "                  solution_add_mem_traffic, (baseline_add_mem_traffic - solution_add_mem_traffic) / float(baseline_add_mem_traffic) * 100.))\n",
    "\n",
    "    curr_log = [(i + 1), current_temp, cost_solution, solution_latency, solution_energy, solution_add_mem_traffic]\n",
    "    logs.append(curr_log)\n",
    "    i += 1\n",
    "    \n",
    "    if current_temp < final_temp:\n",
    "        break\n",
    "        \n",
    "print(\"Execution time: {}s\".format(time.time() - start_time))\n",
    "\n",
    "# dump to csv file\n",
    "with open(os.path.join(base_dir, timeloop_dir, 'joint_topk', sub_dir, 'SA_{}_top{}_summary.csv'.format(cooling_scheduler, topk)), 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(csv_header)\n",
    "    writer.writerows(logs)\n",
    "    \n",
    "# dump best state & solution state to yaml file\n",
    "state = {'best': best_state, 'final': solution_state}\n",
    "with open(os.path.join(base_dir, timeloop_dir, 'joint_topk', sub_dir, 'SA_{}_state_annealing_only.yaml'.format(cooling_scheduler)), 'w') as f:\n",
    "    _ = yaml.dump(state, f)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0ac00f",
   "metadata": {},
   "source": [
    "Copy the best states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5679384",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(base_dir, timeloop_dir, 'joint_topk', sub_dir, 'SA_{}_state.yaml'.format('linear')), 'r') as f:\n",
    "    states = yaml.safe_load(f)\n",
    "    best_state = states['best']\n",
    "\n",
    "# move the best solution result\n",
    "for layer_idx in range(1, n_layers + 1):\n",
    "    loopnest_id = best_state[layer_idx - 1]\n",
    "    src = os.path.join(base_dir, timeloop_dir, 'joint_topk', sub_dir, 'layer{}'.format(layer_idx), \\\n",
    "                       'eval{}'.format(loopnest_id))\n",
    "    src_files = os.listdir(src)\n",
    "    for file in src_files:\n",
    "        file_name = os.path.join(src, file)\n",
    "        if os.path.isfile(file_name):\n",
    "            shutil.copy(file_name, os.path.join(os.path.join(base_dir, timeloop_dir, 'joint_topk', sub_dir, 'layer{}'.format(layer_idx))))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52b7ec1",
   "metadata": {},
   "source": [
    "Generate stats.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edc3217",
   "metadata": {},
   "outputs": [],
   "source": [
    "from authblock_assignment import AuthBlockAssignment\n",
    "\n",
    "AuthBlockAssignment(n_layers, layer_info, \\\n",
    "                    base_dir, timeloop_dir, top_dir, sub_dir, \\\n",
    "                    configuration_dict, \\\n",
    "                    mode=\"search\", \\\n",
    "                    joint=True, generate_summary=True, return_cost_dict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3680d00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cost, rehash, _ = AuthBlockAssignment(n_layers, layer_info, \\\n",
    "                    base_dir, timeloop_dir, top_dir, sub_dir, \\\n",
    "                    configuration_dict, \\\n",
    "                    mode=\"search\", \\\n",
    "                    joint=True, generate_summary=False, return_cost_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397ad310",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, n_layers+1):\n",
    "    print(cost[i]['timeloop_energy'], current_cost_dict[i]['timeloop_energy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca91e336",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in rehash.keys():\n",
    "    print(rehash[key]['total_energy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1216500b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
