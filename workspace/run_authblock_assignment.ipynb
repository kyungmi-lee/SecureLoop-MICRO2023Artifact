{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2369f42e",
   "metadata": {},
   "source": [
    "## Step 2: AuthBlock Assignment\n",
    "\n",
    "Run authentication block assignment for a given loopnest schedule for each layer. In this notebook, we use the per-layer top-1 loopnest schedule and run the authentication block assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45baae41",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import yaml\n",
    "import shutil\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0daa04a",
   "metadata": {},
   "source": [
    "First, we have to define the architecture and the workload similar as before.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32861ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration_dict = {}\n",
    "\n",
    "# template design (with constraints and memory hierarchy representing \"dataflow\")\n",
    "configuration_dict['TEMPLATE_DESIGN'] = 'eyeriss_like'\n",
    "\n",
    "# number of bits used for I/O/W; we assume integer\n",
    "configuration_dict['WORDBITS'] = 16\n",
    "\n",
    "# DRAM bandwidth setting: words / cycle (not bits / cycle)\n",
    "configuration_dict['DRAM_READ_BANDWIDTH'] = 32\n",
    "configuration_dict['DRAM_WRITE_BANDWIDTH'] = 32\n",
    "\n",
    "# SRAM setting\n",
    "# - do we have a single shared glb or multiple glbs for each datatype? \n",
    "# - for each glb (if shared, just one), define depth/width/#banks and bandwidths\n",
    "configuration_dict['SRAM_SHARED'] = True\n",
    "configuration_dict['SRAM_DEPTH'] = [2 ** 13]\n",
    "configuration_dict['SRAM_WIDTH'] = [2 ** 7]\n",
    "configuration_dict['SRAM_BANKS'] = [32]                     # SRAM width and SRAM banks define the maximum possible bandwidth\n",
    "configuration_dict['SRAM_READ_BANDWIDTH'] = [32]\n",
    "configuration_dict['SRAM_WRITE_BANDWIDTH'] = [32]\n",
    "\n",
    "# PE array setting\n",
    "# - shape of PE array X x Y\n",
    "# - whether a PE has a shared scratchpad or separate scratchpads for each datatype\n",
    "configuration_dict['PE_X'] = 14\n",
    "configuration_dict['PE_Y'] = 12\n",
    "configuration_dict['PE_SPAD_SHARED'] = False\n",
    "configuration_dict['PE_SPAD_DEPTH'] = [192, 12, 16]         # Weight, IFmap, OFmap\n",
    "configuration_dict['PE_SPAD_WIDTH'] = [16, 16, 16]\n",
    "\n",
    "# Cryptographic engine setting\n",
    "# - type of cryptographic engine + dram (LPDDR4 + AES-GCM)\n",
    "# - cycle for AES-GCM \n",
    "# - whether the cryptographic engines are shared among all datatypes or assigned to each datatype\n",
    "configuration_dict['CRYPT_ENGINE_TYPE'] = 'effective_lpddr4_aesgcm'\n",
    "configuration_dict['CRYPT_ENGINE_CYCLE_PER_BLOCK'] = 11            # avg. cycle/128bit\n",
    "\n",
    "configuration_dict['CRYPT_ENGINE_SHARED'] = False\n",
    "configuration_dict['CRYPT_ENGINE_COUNT'] = [1, 1, 1]\n",
    "\n",
    "configuration_dict['EFFECTIVE_CONSERVATIVE'] = True\n",
    "\n",
    "# Create directory for this configuration if it doesn't exist already\n",
    "# iterate through design folders to check if any pre-exisiting folder\n",
    "design_dir = 'designs/{}'.format(configuration_dict['TEMPLATE_DESIGN'])\n",
    "arch_dir = None\n",
    "total_vers = 0\n",
    "for path in os.listdir(design_dir):\n",
    "    if path != 'template' and os.path.isdir(os.path.join(design_dir, path)):\n",
    "        try:\n",
    "            with open(os.path.join(design_dir, path, 'config.yaml'), 'r') as f:\n",
    "                config_file = yaml.safe_load(f)\n",
    "            total_vers += 1\n",
    "            if config_file == configuration_dict:\n",
    "                arch_dir = path\n",
    "                print(\"Pre-existing folder found. Setting the arch_dir to {}\".format(arch_dir))\n",
    "                break\n",
    "        except:\n",
    "            print(\"No config.yaml file in the directory {}\".format(str(os.path.join(design_dir, path))))\n",
    "            \n",
    "if arch_dir == None:\n",
    "    raise NameError(\"Architecture is not found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81f4ead",
   "metadata": {},
   "source": [
    "..else if you know which folder you want to use, specify here instead of running the above cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e979902",
   "metadata": {},
   "outputs": [],
   "source": [
    "design_dir = 'designs/{}'.format('eyeriss_like') # define your design name here\n",
    "\n",
    "arch_ver = 0\n",
    "arch_dir = 'ver{}'.format(arch_ver)              # sub directory under designs/{name}/{arch_dir}\n",
    "with open(os.path.join(design_dir, arch_dir, 'config.yaml'), 'r') as f:\n",
    "    configuration_dict = yaml.safe_load(f)\n",
    "print(\"Setting the architecture directory to: {}\".format(os.path.join(design_dir, arch_dir)))\n",
    "print(\"Printing configuration:\")\n",
    "for key, value in configuration_dict.items():\n",
    "    print(\"{}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69406e6",
   "metadata": {},
   "source": [
    "Define the workload here. Skip the pytorch2timeloop conversion (should be done when generating loopnests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a6b6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision.models as model_zoo\n",
    "\n",
    "import pytorch2timeloop as pytorch2timeloop\n",
    "\n",
    "# Note: this version only supports nn.Conv2d (both normal convs and depthwise/pointwise convs) and nn.Linear\n",
    "\n",
    "# AlexNet\n",
    "model_name = 'alexnet'\n",
    "net = model_zoo.alexnet(pretrained=False)\n",
    "\n",
    "# ResNet18\n",
    "# model_name = 'resnet18'\n",
    "# net = model_zoo.resnet18(pretrained=False)\n",
    "\n",
    "# MobilenetV2\n",
    "# model_name = 'mobilenet_v2'\n",
    "# net = model_zoo.mobilenet_v2(pretrained=False)\n",
    "\n",
    "# Input / Batch info\n",
    "input_size = (3, 224, 224)\n",
    "batch_size = 1\n",
    "\n",
    "print(net)\n",
    "\n",
    "top_dir = 'workloads'\n",
    "sub_dir = '{}_batch{}'.format(model_name, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e7c211",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = Path(os.getcwd())\n",
    "timeloop_dir = 'designs/{}/{}'.format(configuration_dict['TEMPLATE_DESIGN'], arch_dir)\n",
    "\n",
    "n_layers = 0\n",
    "layer_dict = {}\n",
    "layer_duplicate_info = {}\n",
    "unique_layers = []\n",
    "for module in net.modules():\n",
    "    if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n",
    "        n_layers += 1\n",
    "        if n_layers not in layer_dict.keys():\n",
    "            workload_path = os.path.join(base_dir, top_dir, sub_dir, '{}_layer{}.yaml'.format(sub_dir, n_layers))\n",
    "            with open(workload_path, 'r') as f:\n",
    "                workload_info = yaml.safe_load(f)\n",
    "            layer_dict[n_layers] = workload_info\n",
    "        \n",
    "        # identify the earliest duplicate layer\n",
    "        for key in range(1, n_layers):\n",
    "            if layer_dict[key] == layer_dict[n_layers]:\n",
    "                layer_duplicate_info[n_layers] = key\n",
    "                break\n",
    "        if n_layers not in layer_duplicate_info:\n",
    "            unique_layers.append(n_layers)\n",
    "            \n",
    "print(layer_duplicate_info)\n",
    "print(unique_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369b374a",
   "metadata": {},
   "source": [
    "### Generate a DNN workload layer dependency dictionary\n",
    "\n",
    "We need to take care of interlayer dependencies. Since the layer number does not imply the actual layer order/dependency, and often multiple layers can be dependent on one layer and vice versa (e.g., residual connections), we create a back-propagation graph and determine the dependency from the graph. \n",
    "\n",
    "Depending on your accelerator architecture assumptions, some post-processing operations can be performed on-the-fly. Here, we assume that ReLU activation and batch normalization can be performed on-the-fly (thus, not affect the dependency), while pooling operations and adding multiple feature maps together (e.g., adding residual branches together) cannot be done on-the-fly (thus, break the depenency).\n",
    "\n",
    "If you have different assumptions about the post-processing or using different versions of PyTorch with different backend operations, `BackpropGraph.isDependentLayer` has to be modified accordingly.\n",
    "\n",
    "Define whether you want to ignore the interlayer dependency entirely (i.e., always use rehashing). Then, the dependency dictionary will ignore all interlayer dependency. \n",
    "\n",
    "Finally, constructing and analyzing the back-propagation grpah can take > 5 minutes for deep models like MobilenetV2. Since this dictionary has to be constructed only once per each model, we first search for existing dictionary. If not, then we generate a graph and create a dictionary and save it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a71fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "ignore_interlayer = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f2a942",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_layer_dependency_utils import BackpropGraph\n",
    "\n",
    "workload_path = os.path.join(base_dir, top_dir, sub_dir, 'layer_info_{}.yaml'.format('ignore_interlayer' if ignore_interlayer \\\n",
    "                                                                                     else 'interlayer'))\n",
    "try:\n",
    "    with open(workload_path, 'r') as f:\n",
    "        layer_info = yaml.safe_load(f)\n",
    "    for layer_idx in range(1, n_layers + 1):\n",
    "        print(layer_idx, layer_info[layer_idx])\n",
    "except:\n",
    "\n",
    "    graph = BackpropGraph(net, [1, input_size[0], input_size[1], input_size[2]])\n",
    "    consecutive_dict, dependent_dict = graph.get_dependency_info()\n",
    "\n",
    "    # construct layer_info\n",
    "    layer_info = {}\n",
    "    for layer_idx in range(1, n_layers + 1):\n",
    "        info = {}\n",
    "        if layer_idx in unique_layers:\n",
    "            info['layer_id_for_timeloop'] = layer_idx\n",
    "        else:\n",
    "            info['layer_id_for_timeloop'] = layer_duplicate_info[layer_idx]\n",
    "        info['prev_layer'] = []\n",
    "        info['next_layer'] = []\n",
    "        info['dependent_prev_layer'] = []\n",
    "        info['dependent_next_layer'] = []\n",
    "        layer_info[layer_idx] = info\n",
    "\n",
    "    for layer_idx in range(1, n_layers + 1):\n",
    "        consecutive = consecutive_dict[layer_idx]\n",
    "        dependent = dependent_dict[layer_idx]\n",
    "        layer_info[layer_idx]['next_layer'].extend(consecutive)\n",
    "        for i in consecutive:\n",
    "            layer_info[i]['prev_layer'].append(layer_idx)\n",
    "        if len(dependent) > 0 and not ignore_interlayer:\n",
    "            layer_info[layer_idx]['dependent_next_layer'].extend(dependent)   \n",
    "            for i in dependent:\n",
    "                layer_info[i]['dependent_prev_layer'].append(layer_idx)\n",
    "\n",
    "    for layer_idx in range(1, n_layers + 1):\n",
    "        print(layer_idx, layer_info[layer_idx])\n",
    "\n",
    "    # store therresults - this can take long for deep models like MobileNet..\n",
    "    # f\"{base_dir/top_dir/sub_dir/sub_dir}_layer{layer_id}.yaml \"\n",
    "    with open(workload_path, 'w') as f:\n",
    "        _ = yaml.dump(layer_info, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1f321c",
   "metadata": {},
   "source": [
    "### Define hyperparameters for the AuthBlock assignment\n",
    "\n",
    "Define whether you want to use 1) a fixed AuthBlock size and orientation, 2) tile-as-an-AuthBlock, or 3) search for the optimal AuthBlock assignment. For this experiment, we assume that the tag size is 64 bits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf6d68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "WORD_SIZE = configuration_dict['WORDBITS']\n",
    "TAG_SIZE = 64\n",
    "\n",
    "# Define which method you want to use for AuthBlock assignment\n",
    "# mode = \"fixed\" \n",
    "# mode = \"tile\"\n",
    "mode = \"search\"\n",
    "\n",
    "# If you want to use a fixed assignment, define the size and orientation here:\n",
    "# authblock_size = 8                     # in words\n",
    "# authblock_orientation = [3, 2, 1, 0]   # [column, row, in_channel, out_channel] --> column is the most innermost loop\n",
    "\n",
    "# If you want to skip the search for certain layers (e.g., non-convolution layers in AlexNet), define here:\n",
    "# search_input_layers_except = []\n",
    "\n",
    "# Print all stats?\n",
    "print_all_stats = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d263e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_block_size = {}\n",
    "predefined_u = {}\n",
    "predefined_perm = {}\n",
    "\n",
    "if mode == \"fixed\":\n",
    "    assert (authblock_size > 1 and authblock_orientation != None)\n",
    "    search_flag = False\n",
    "    predefined_u_value = authblock_size\n",
    "    predefined_perm_value = authblock_orientation\n",
    "        \n",
    "elif mode == \"tile\":\n",
    "    search_flag = False\n",
    "    predefined_u_value = 'tile'\n",
    "    predefined_perm_value = None\n",
    "    \n",
    "elif mode == \"search\":\n",
    "    search_flag = True\n",
    "    predefined_u_value = 'tile'\n",
    "    predefined_perm_value = None\n",
    "    \n",
    "for layer_id in range(1, n_layers+1):\n",
    "    search_block_size[layer_id] = search_flag\n",
    "    predefined_u[layer_id] = {}\n",
    "    predefined_perm[layer_id] = {}\n",
    "    predefined_u[layer_id]['W'] = predefined_u_value\n",
    "    predefined_u[layer_id]['I'] = predefined_u_value if mode != \"search\" else -1\n",
    "    predefined_u[layer_id]['O'] = predefined_u_value\n",
    "    predefined_perm[layer_id]['W'] = predefined_perm_value\n",
    "    predefined_perm[layer_id]['I'] = predefined_perm_value\n",
    "    predefined_perm[layer_id]['O'] = predefined_perm_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc1923a",
   "metadata": {},
   "source": [
    "### Generate memory traffic dict based on your choice of AuthBlock assignment method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb71b6b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from testbench_utils import generate_memory_traffic_dict\n",
    "\n",
    "memory_traffic_dict, block_info_dict = generate_memory_traffic_dict(n_layers, layer_info, predefined_u, predefined_perm, \\\n",
    "                                                                    base_dir, timeloop_dir, top_dir, sub_dir, \\\n",
    "                                                                    search_block_size, u_multiple_of=128//WORD_SIZE, \\\n",
    "                                                                    WORD_SIZE=WORD_SIZE, TAG_SIZE=TAG_SIZE)\n",
    "\n",
    "if print_all_stats:\n",
    "    for layer_id in range(1, n_layers + 1):\n",
    "        w_memory_traffic = memory_traffic_dict[layer_id]['W']\n",
    "        i_memory_traffic = memory_traffic_dict[layer_id]['I']\n",
    "        o_memory_traffic = memory_traffic_dict[layer_id]['O']\n",
    "\n",
    "        w_block_info = block_info_dict[layer_id]['W']\n",
    "        i_block_info = block_info_dict[layer_id]['I']\n",
    "        o_block_info = block_info_dict[layer_id]['O']\n",
    "\n",
    "        print(\"\\nLayer {} stats:\".format(layer_id))\n",
    "        print(\"\\n--Layer info: layer id in timeloop is {}\".format(layer_info[layer_id]['layer_id_for_timeloop']))\n",
    "        print(\"\\n--Weights:\")\n",
    "        print(\"----Base read: {}\".format(w_memory_traffic['base_read']))\n",
    "        print(\"----Base write: {}\".format(w_memory_traffic['base_write']))\n",
    "        print(\"----Redundant read: {}\".format(w_memory_traffic['redundant_read']))\n",
    "        print(\"----Redundant write: {}\".format(w_memory_traffic['redundant_write']))\n",
    "        print(\"----Tag read: {}\".format(w_memory_traffic['tag_read']))\n",
    "        print(\"----Tag write: {}\".format(w_memory_traffic['tag_write']))\n",
    "        print(\"----Auth block size: {}\".format(w_block_info['u']))\n",
    "\n",
    "        print(\"\\n--Inputs:\")\n",
    "        print(\"----Base read: {}\".format(i_memory_traffic['base_read']))\n",
    "        print(\"----Base write: {}\".format(i_memory_traffic['base_write']))\n",
    "        print(\"----Redundant read: {}\".format(i_memory_traffic['redundant_read']))\n",
    "        print(\"----Redundant write: {}\".format(i_memory_traffic['redundant_write']))\n",
    "        print(\"----Tag read: {}\".format(i_memory_traffic['tag_read']))\n",
    "        print(\"----Tag write: {}\".format(i_memory_traffic['tag_write']))\n",
    "        print(\"----Auth block size: {}\".format(i_block_info['u']))\n",
    "        print(\"----Auth tile read permutation: {}\".format(i_block_info['permutation']))\n",
    "        if 'shared' in i_block_info.keys():\n",
    "            print(\"----Auth block assignment shared layers: {}\".format(i_block_info['shared']))\n",
    "        if 'reference_layer' in i_block_info.keys():\n",
    "            print(\"----Reference layer among shared layers: {}\".format(i_block_info['reference_layer']))\n",
    "\n",
    "        print(\"\\n--Outputs:\")\n",
    "        print(\"----Base read: {}\".format(o_memory_traffic['base_read']))\n",
    "        print(\"----Base write: {}\".format(o_memory_traffic['base_write']))\n",
    "        print(\"----Redundant read: {}\".format(o_memory_traffic['redundant_read']))\n",
    "        print(\"----Redundant write: {}\".format(o_memory_traffic['redundant_write']))\n",
    "        print(\"----Tag read: {}\".format(o_memory_traffic['tag_read']))\n",
    "        print(\"----Tag write: {}\".format(o_memory_traffic['tag_write']))\n",
    "        print(\"----Auth block size: {}\".format(o_block_info['u']))\n",
    "\n",
    "        print(\"\\n===============================================================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22788f78",
   "metadata": {},
   "source": [
    "### Generate rehash information dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154a3fc4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from testbench_utils import generate_rehash_info_dict\n",
    "rehash_info_dict = generate_rehash_info_dict(n_layers, layer_info, block_info_dict, \\\n",
    "                                             base_dir, timeloop_dir, top_dir, sub_dir)\n",
    "\n",
    "if print_all_stats:\n",
    "    for key in rehash_info_dict.keys():\n",
    "        print(\"Rehashing between layers {} - {}\".format(key[0], key[1]))\n",
    "        print(\"--Base read (encrypted ofmap read of prev layer): {}\".format(rehash_info_dict[key]['base_read']))\n",
    "        print(\"--Tag read (tags to check the integrity when reading the ofmap of prev layer): {}\".format(rehash_info_dict[key]['tag_read']))\n",
    "        print(\"--Base write (once we re-encrypt the data according to ifmap of next layer): {}\".format(rehash_info_dict[key]['base_write']))\n",
    "        print(\"--Tag write (we have to update tags according to the new blocks): {}\".format(rehash_info_dict[key]['tag_write']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8854d4b",
   "metadata": {},
   "source": [
    "### Convert memory traffic information to cryptographic action counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cd2d0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from testbench_utils import get_action_dict, get_action_dict_for_rehash\n",
    "cryptographic_action_count_dict = get_action_dict(n_layers, memory_traffic_dict, block_info_dict, \\\n",
    "                                                  WORD_SIZE, TAG_SIZE, AES_DATAPATH=128)\n",
    "rehash_action_count_dict = get_action_dict_for_rehash(rehash_info_dict, block_info_dict, WORD_SIZE, TAG_SIZE, AES_DATAPATH=128)\n",
    "\n",
    "if print_all_stats:\n",
    "    print(\"Layers\\n\\n\")\n",
    "    for layer_id in range(1, n_layers + 1):\n",
    "        print(\"\\nLayer {} stats:\".format(layer_id))\n",
    "\n",
    "        weight_count = cryptographic_action_count_dict[layer_id]['W']\n",
    "        print(\"\\n--Weights:\")\n",
    "        print(\"----AES encryption/decryption count: {}\".format(weight_count['aes_engine_count']))\n",
    "        print(\"----GF multiplication count: {}\".format(weight_count['gf_mult_count']))\n",
    "        print(\"----XOR count: {}\".format(weight_count['xor_count']))\n",
    "        print(\"----Additional memory read (bits): {}\".format(weight_count['additional_read_bits']))\n",
    "        print(\"----Additional memory write (bits): {}\".format(weight_count['additional_write_bits']))\n",
    "\n",
    "        input_count = cryptographic_action_count_dict[layer_id]['I']\n",
    "        print(\"\\n--Inputs:\")\n",
    "        print(\"----AES encryption/decryption count: {}\".format(input_count['aes_engine_count']))\n",
    "        print(\"----GF multiplication count: {}\".format(input_count['gf_mult_count']))\n",
    "        print(\"----XOR count: {}\".format(input_count['xor_count']))\n",
    "        print(\"----Additional memory read (bits): {}\".format(input_count['additional_read_bits']))\n",
    "        print(\"----Additional memory write (bits): {}\".format(input_count['additional_write_bits']))\n",
    "\n",
    "        output_count = cryptographic_action_count_dict[layer_id]['O']\n",
    "        print(\"\\n--Outputs:\")\n",
    "        print(\"----AES encryption/decryption count: {}\".format(output_count['aes_engine_count']))\n",
    "        print(\"----GF multiplication count: {}\".format(output_count['gf_mult_count']))\n",
    "        print(\"----XOR count: {}\".format(output_count['xor_count']))\n",
    "        print(\"----Additional memory read (bits): {}\".format(output_count['additional_read_bits']))\n",
    "        print(\"----Additional memory write (bits): {}\".format(output_count['additional_write_bits']))\n",
    "    \n",
    "    print(\"\\n\\nRehash\\n\\n\")\n",
    "    for key in rehash_action_count_dict:\n",
    "        print(\"\\nRehash for layer {} - {}:\".format(key[0], key[1]))\n",
    "        print(\"--AES encryption/decryption count: {}\".format(rehash_action_count_dict[key]['aes_engine_count']))\n",
    "        print(\"--GF multiplication count: {}\".format(rehash_action_count_dict[key]['gf_mult_count']))\n",
    "        print(\"--XOR count: {}\".format(rehash_action_count_dict[key]['xor_count']))\n",
    "        print(\"--Additional memory read (bits): {}\".format(rehash_action_count_dict[key]['additional_read_bits']))\n",
    "        print(\"--Additional memory write (bits): {}\".format(rehash_action_count_dict[key]['additional_write_bits']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53725b0",
   "metadata": {},
   "source": [
    "### Generate a final stat.csv file with estimated latency & energy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab656648",
   "metadata": {},
   "source": [
    "This is energy / latency estimate form Banerjee (2017) for AES-GCM engine implementations in 40nm. We ignore the XOR energy for now. If you want to use different energy / latency profile for AES-GCM (+ XOR), define it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079ef300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fully pipelined AES-GCM: 1 cycle / 1 cycle\n",
    "if configuration_dict['CRYPT_ENGINE_CYCLE_PER_BLOCK'] == 1:\n",
    "    AESGCM_energy_profile = {'AES': 1.29 * 128, 'GCM': 57.7, 'XOR': 0} # pJ / op (e.g., 128-bit AES, 128-bit * 128-bit GCM)\n",
    "    AESGCM_latency_profile = {'AES': 1, 'GCM': 1, 'XOR': 1} # cycle / op \n",
    "\n",
    "# Parallel AES-GCM: 11 cycle / 8 cycle\n",
    "elif configuration_dict['CRYPT_ENGINE_CYCLE_PER_BLOCK'] == 11:\n",
    "    AESGCM_energy_profile = {'AES': 1.52 * 128, 'GCM': 82.4, 'XOR': 0} # pJ / op (e.g., 128-bit AES, 128-bit * 128-bit GCM)\n",
    "    AESGCM_latency_profile = {'AES': 11, 'GCM': 8, 'XOR': 1} # cycle / op \n",
    "    \n",
    "# Serial AES-GCM: 336 cycle / 128 cycle\n",
    "elif configuration_dict['CRYPT_ENGINE_CYCLE_PER_BLOCK'] == 336:\n",
    "    AESGCM_energy_profile = {'AES': 6 * 128, 'GCM': 345.6, 'XOR': 0} # pJ / op (e.g., 128-bit AES, 128-bit * 128-bit GCM)\n",
    "    AESGCM_latency_profile = {'AES': 336, 'GCM': 128, 'XOR': 1} # cycle / op "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9eaf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "evaluation_folder = 'evaluation'\n",
    "\n",
    "# For LPDDR4\n",
    "MEMORY_READ_PER_BIT_ENERGY = 8\n",
    "MEMORY_WRITE_PER_BIT_ENERGY = 8\n",
    "\n",
    "summaries = []\n",
    "for layer_id in range(1, n_layers + 1):\n",
    "    summary = [layer_id]\n",
    "    layer_id_for_timeloop = layer_info[layer_id]['layer_id_for_timeloop']\n",
    "    stats_file = os.path.join(base_dir, timeloop_dir, evaluation_folder, sub_dir, \"layer{}\".format(layer_id_for_timeloop), \\\n",
    "                              \"timeloop-model.stats.txt\")\n",
    "    with open(stats_file, 'r') as f:\n",
    "        lines = f.read().split('\\n')[-200:]\n",
    "        for line in lines:\n",
    "            if line.startswith('Energy'):\n",
    "                energy = eval(line.split(': ')[1].split(' ')[0]) * float(10**6) # micro to pico\n",
    "            elif line.startswith('Cycles'):\n",
    "                cycle = eval(line.split(': ')[1])\n",
    "    \n",
    "    summary.extend([cycle, energy])\n",
    "    \n",
    "    total_memory_read_bits = 0\n",
    "    total_memory_write_bits = 0\n",
    "    additional_read_bits = 0\n",
    "    additional_write_bits = 0\n",
    "    tag_bits = 0\n",
    "    redundant_bits = 0\n",
    "    \n",
    "    aes_counts = []\n",
    "    gcm_counts = []\n",
    "    xor_counts = []\n",
    "    \n",
    "    # weights\n",
    "    weight_memory_traffic_dict = memory_traffic_dict[layer_id]['W']\n",
    "    weight_block_info_dict = block_info_dict[layer_id]['W']\n",
    "    weight_action_count_dict = cryptographic_action_count_dict[layer_id]['W']\n",
    "    \n",
    "    total_memory_read_bits += weight_action_count_dict['total_read_bits']\n",
    "    total_memory_write_bits += weight_action_count_dict['total_write_bits']\n",
    "    \n",
    "    additional_read_bits +=  weight_action_count_dict['additional_read_bits']\n",
    "    additional_write_bits += weight_action_count_dict['additional_write_bits']\n",
    "    \n",
    "    tag_bits += weight_action_count_dict['tag_bits']\n",
    "    redundant_bits += weight_action_count_dict['redundant_bits']\n",
    "    \n",
    "    # aes counts, gcm_counts, xor_counts, additional_read_bits, additional_write_bits, u\n",
    "    summary.extend([weight_action_count_dict['aes_engine_count'], \\\n",
    "                    weight_action_count_dict['gf_mult_count'], \\\n",
    "                    weight_action_count_dict['xor_count'], \\\n",
    "                    weight_action_count_dict['additional_read_bits'], \\\n",
    "                    weight_action_count_dict['additional_write_bits'], \\\n",
    "                    weight_block_info_dict['u'], \\\n",
    "                    weight_action_count_dict['tag_bits'], \\\n",
    "                    weight_action_count_dict['redundant_bits']])\n",
    "    \n",
    "    aes_counts.append(weight_action_count_dict['aes_engine_count'])\n",
    "    gcm_counts.append(weight_action_count_dict['gf_mult_count'])\n",
    "    xor_counts.append(weight_action_count_dict['xor_count'])\n",
    "    \n",
    "    # inputs\n",
    "    input_memory_traffic_dict = memory_traffic_dict[layer_id]['I']\n",
    "    input_block_info_dict = block_info_dict[layer_id]['I']\n",
    "    input_action_count_dict = cryptographic_action_count_dict[layer_id]['I']\n",
    "    \n",
    "    total_memory_read_bits += input_action_count_dict['total_read_bits']\n",
    "    total_memory_write_bits += input_action_count_dict['total_write_bits']\n",
    "    \n",
    "    additional_read_bits += input_action_count_dict['additional_read_bits']\n",
    "    additional_write_bits += input_action_count_dict['additional_write_bits']\n",
    "    \n",
    "    tag_bits += input_action_count_dict['tag_bits']\n",
    "    redundant_bits += input_action_count_dict['redundant_bits']\n",
    "    \n",
    "    # aes counts, gcm_counts, xor_counts, additional_read_bits, additional_write_bits, u\n",
    "    summary.extend([input_action_count_dict['aes_engine_count'], \\\n",
    "                    input_action_count_dict['gf_mult_count'], \\\n",
    "                    input_action_count_dict['xor_count'], \\\n",
    "                    input_action_count_dict['additional_read_bits'], \\\n",
    "                    input_action_count_dict['additional_write_bits'], \\\n",
    "                    input_block_info_dict['u'], \\\n",
    "                    input_block_info_dict['permutation'], \\\n",
    "                    input_action_count_dict['tag_bits'], \\\n",
    "                    input_action_count_dict['redundant_bits']])\n",
    "    \n",
    "    aes_counts.append(input_action_count_dict['aes_engine_count'])\n",
    "    gcm_counts.append(input_action_count_dict['gf_mult_count'])\n",
    "    xor_counts.append(input_action_count_dict['xor_count'])\n",
    "    \n",
    "    # outputs\n",
    "    output_memory_traffic_dict = memory_traffic_dict[layer_id]['O']\n",
    "    output_block_info_dict = block_info_dict[layer_id]['O']\n",
    "    output_action_count_dict = cryptographic_action_count_dict[layer_id]['O']\n",
    "    \n",
    "    total_memory_read_bits += output_action_count_dict['total_read_bits']\n",
    "    total_memory_write_bits += output_action_count_dict['total_write_bits']\n",
    "    \n",
    "    additional_read_bits += output_action_count_dict['additional_read_bits']\n",
    "    additional_write_bits += output_action_count_dict['additional_write_bits']\n",
    "    \n",
    "    tag_bits += output_action_count_dict['tag_bits']\n",
    "    redundant_bits += output_action_count_dict['redundant_bits']\n",
    "    \n",
    "    # aes counts, gcm_counts, xor_counts, additional_read_bits, additional_write_bits, u\n",
    "    summary.extend([output_action_count_dict['aes_engine_count'], \\\n",
    "                    output_action_count_dict['gf_mult_count'], \\\n",
    "                    output_action_count_dict['xor_count'], \\\n",
    "                    output_action_count_dict['additional_read_bits'], \\\n",
    "                    output_action_count_dict['additional_write_bits'], \\\n",
    "                    output_block_info_dict['u'], \\\n",
    "                    output_action_count_dict['tag_bits'], \\\n",
    "                    output_action_count_dict['redundant_bits']])\n",
    "    \n",
    "    aes_counts.append(output_action_count_dict['aes_engine_count'])\n",
    "    gcm_counts.append(output_action_count_dict['gf_mult_count'])\n",
    "    xor_counts.append(output_action_count_dict['xor_count'])\n",
    "    \n",
    "    # get crypto-latency for this layer\n",
    "    if configuration_dict['CRYPT_ENGINE_SHARED']:\n",
    "        aes_latency = sum(aes_counts) * (AESGCM_latency_profile['AES'] + AESGCM_latency_profile['XOR']) \\\n",
    "                      / sum(configuration_dict['CRYPT_ENGINE_COUNT'])\n",
    "        gcm_latency = sum(gcm_counts) * (AESGCM_latency_profile['GCM'] + AESGCM_latency_profile['XOR']) \\\n",
    "                      / sum(configuration_dict['CRYPT_ENGINE_COUNT'])\n",
    "        crypt_latency = max(aes_latency, gcm_latency) # assuming AES and GF can be pipelined\n",
    "    else:\n",
    "        aes_latency = [aes_counts[i] * (AESGCM_latency_profile['AES'] + AESGCM_latency_profile['XOR']) \\\n",
    "                       / (configuration_dict['CRYPT_ENGINE_COUNT'][i]) for i in range(3)]\n",
    "        gcm_latency = [gcm_counts[i] * (AESGCM_latency_profile['GCM'] + AESGCM_latency_profile['XOR']) \\\n",
    "                       / (configuration_dict['CRYPT_ENGINE_COUNT'][i]) for i in range(3)]\n",
    "        crypt_latency = max([max(aes_latency[i], gcm_latency[i]) for i in range(3)])\n",
    "    \n",
    "    memory_latency = max(total_memory_read_bits / (configuration_dict['DRAM_READ_BANDWIDTH'] * configuration_dict['WORDBITS']), \\\n",
    "                         total_memory_write_bits / (configuration_dict['DRAM_WRITE_BANDWIDTH'] * configuration_dict['WORDBITS']))\n",
    "    total_latency = max(cycle, crypt_latency, memory_latency)\n",
    "    \n",
    "    summary.extend([crypt_latency, memory_latency, total_latency])\n",
    "        \n",
    "    # get crypto-energy for this layer\n",
    "    aes_energy = sum(aes_counts) * AESGCM_energy_profile['AES']\n",
    "    gcm_energy = sum(gcm_counts) * AESGCM_energy_profile['GCM']\n",
    "    xor_energy = sum(xor_counts) * AESGCM_energy_profile['XOR']\n",
    "    \n",
    "    memory_energy = additional_read_bits * MEMORY_READ_PER_BIT_ENERGY + additional_write_bits * MEMORY_WRITE_PER_BIT_ENERGY\n",
    "    total_energy = energy + (aes_energy + gcm_energy + xor_energy) + memory_energy\n",
    "    \n",
    "    additional_mem_traffic = additional_read_bits + additional_write_bits\n",
    "    \n",
    "    summary.extend([(aes_energy + gcm_energy + xor_energy), memory_energy, total_energy])\n",
    "    summary.extend([additional_mem_traffic, tag_bits, redundant_bits])\n",
    "    \n",
    "    summaries.append(summary)\n",
    "    \n",
    "    # print(key, additional_mem_traffic)\n",
    "\n",
    "for key in rehash_action_count_dict.keys():\n",
    "    summary = [\"Rehash{}-{}\".format(key[0], key[1])]\n",
    "    summary.extend([0, 0, \\\n",
    "                    0, 0, 0, 0, 0, 0, 0, 0, \\\n",
    "                    0, 0, 0, 0, 0, 0, 0, 0, 0, \\\n",
    "                    0, 0, 0, 0, 0, 0, 0, 0])\n",
    "    \n",
    "    # when there are non-shared AES-GCM engine\n",
    "    # rehashing only for ifmap - ofmap\n",
    "    aes_latency = rehash_action_count_dict[key]['aes_engine_count'] * (AESGCM_latency_profile['AES'] + AESGCM_latency_profile['XOR']) \\\n",
    "                  / (configuration_dict['CRYPT_ENGINE_COUNT'][1] + configuration_dict['CRYPT_ENGINE_COUNT'][2])\n",
    "    gcm_latency = rehash_action_count_dict[key]['gf_mult_count'] * (AESGCM_latency_profile['GCM'] + AESGCM_latency_profile['XOR']) \\\n",
    "                  / (configuration_dict['CRYPT_ENGINE_COUNT'][1] + configuration_dict['CRYPT_ENGINE_COUNT'][2])\n",
    "    crypt_latency = max(aes_latency, gcm_latency)\n",
    "    \n",
    "    aes_energy = rehash_action_count_dict[key]['aes_engine_count'] * AESGCM_energy_profile['AES']\n",
    "    gcm_energy = rehash_action_count_dict[key]['gf_mult_count'] * AESGCM_energy_profile['GCM']\n",
    "    xor_energy = rehash_action_count_dict[key]['xor_count'] * AESGCM_energy_profile['XOR']\n",
    "    \n",
    "    memory_latency = max(rehash_action_count_dict[key]['total_read_bits'] / \\\n",
    "                         (configuration_dict['DRAM_READ_BANDWIDTH'] * configuration_dict['WORDBITS']), \\\n",
    "                         rehash_action_count_dict[key]['total_write_bits'] / \\\n",
    "                         (configuration_dict['DRAM_WRITE_BANDWIDTH'] * configuration_dict['WORDBITS']))\n",
    "    memory_energy = rehash_action_count_dict[key]['total_read_bits'] * MEMORY_READ_PER_BIT_ENERGY + \\\n",
    "                    rehash_action_count_dict[key]['total_write_bits'] * MEMORY_WRITE_PER_BIT_ENERGY\n",
    "    \n",
    "    additional_mem_traffic = rehash_action_count_dict[key]['total_read_bits'] + \\\n",
    "                             rehash_action_count_dict[key]['total_write_bits']\n",
    "    \n",
    "    tag_bits = rehash_action_count_dict[key]['additional_read_bits'] + \\\n",
    "               rehash_action_count_dict[key]['additional_write_bits']\n",
    "    \n",
    "    total_latency = max(crypt_latency, memory_latency)\n",
    "    total_energy = (aes_energy + gcm_energy + xor_energy) + memory_energy\n",
    "    \n",
    "    summary.extend([crypt_latency, memory_latency, total_latency, \\\n",
    "                    (aes_energy + gcm_energy + xor_energy), memory_energy, total_energy])\n",
    "    summary.extend([additional_mem_traffic, tag_bits, 0])\n",
    "    \n",
    "    summaries.append(summary)\n",
    "    \n",
    "summary_header = ['Layer#', 'Baseline Cycle', 'Baseline Energy (pJ)', \\\n",
    "                  'Weight AES Count', 'Weight GFMult Count', 'Weight XOR Count', \\\n",
    "                  'Weight Additional Memory Read (bits)', 'Weight Additional Memory Write (bits)', \\\n",
    "                  'Weight Authentication Block Size', 'Weight Tag Bits', 'Weight Redundant Bits', \\\n",
    "                  'Input AES Count', 'Input GFMult Count', 'Input XOR Count', \\\n",
    "                  'Input Additional Memory Read (bits)', 'Input Additional Memory Write (bits)', \\\n",
    "                  'Input Authentication Block Size', 'Input Authentication Permutation', \\\n",
    "                  'Input Tag Bits', 'Input Redundant Bits', \\\n",
    "                  'Output AES Count', 'Output GFMult Count', 'Output XOR Count', \\\n",
    "                  'Output Additional Memory Read (bits)', 'Output Additional Memory Write (bits)', \\\n",
    "                  'Output Authentication Block Size', 'Output Tag Bits', 'Output Redundant Bits', \\\n",
    "                  'CryptEngine Latency', 'Final Memory Read/Write Latency', 'Total Latency', \\\n",
    "                  'CryptEngine Energy (pJ)', 'Additional Memory Read/Write Energy (pJ)', 'Total Energy (pJ)', \\\n",
    "                  'Additional Memory Traffic (bits)', 'Tag Bits', 'Redundant Bits']\n",
    "\n",
    "write_dst = os.path.join(base_dir, timeloop_dir, evaluation_folder, sub_dir, 'stat.csv')\n",
    "with open(write_dst, 'w') as f:\n",
    "    csv_file = csv.writer(f)\n",
    "    csv_file.writerow(summary_header)\n",
    "    for result in summaries:\n",
    "        csv_file.writerow(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ab663f",
   "metadata": {},
   "source": [
    "### Or run them all together with one top-level function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a1c11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from authblock_assignment import AuthBlockAssignment\n",
    "\n",
    "AuthBlockAssignment(n_layers, layer_info, \\\n",
    "                    base_dir, timeloop_dir, top_dir, sub_dir, \\\n",
    "                    configuration_dict, \\\n",
    "                    mode=\"search\", authblock_size=-1, authblock_orientation=None, \\\n",
    "                    joint=False, generate_summary=True, return_cost_dict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbc8f4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
