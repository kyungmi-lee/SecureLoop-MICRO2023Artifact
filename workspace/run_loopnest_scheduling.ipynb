{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6721a88",
   "metadata": {},
   "source": [
    "## Step 1: Loopnest Scheduling\n",
    "\n",
    "Run Timeloop-Topk using either the baseline model or the effective bandwidth model (considering the cryptographic engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56540f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import yaml\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "from utils import generate_arch_files, xml2mapping "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9f5849",
   "metadata": {},
   "source": [
    "### Define the architecture\n",
    "\n",
    "First, define an architecture design. The code below generates/detects a new architecture configuration based on the template design at `designs/{design_name}/template`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82e39c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration_dict = {}\n",
    "\n",
    "# template design (with constraints and memory hierarchy representing \"dataflow\")\n",
    "configuration_dict['TEMPLATE_DESIGN'] = 'eyeriss_like'\n",
    "\n",
    "# number of bits used for I/O/W; we assume integer\n",
    "configuration_dict['WORDBITS'] = 16\n",
    "\n",
    "# DRAM bandwidth setting: words / cycle (not bits / cycle)\n",
    "configuration_dict['DRAM_READ_BANDWIDTH'] = 32\n",
    "configuration_dict['DRAM_WRITE_BANDWIDTH'] = 32\n",
    "\n",
    "# SRAM setting\n",
    "# - do we have a single shared glb or multiple glbs for each datatype? \n",
    "# - for each glb (if shared, just one), define depth/width/#banks and bandwidths\n",
    "configuration_dict['SRAM_SHARED'] = True\n",
    "configuration_dict['SRAM_DEPTH'] = [2 ** 13]\n",
    "configuration_dict['SRAM_WIDTH'] = [2 ** 7]\n",
    "configuration_dict['SRAM_BANKS'] = [32]                     # SRAM width and SRAM banks define the maximum possible bandwidth\n",
    "configuration_dict['SRAM_READ_BANDWIDTH'] = [32]\n",
    "configuration_dict['SRAM_WRITE_BANDWIDTH'] = [32]\n",
    "\n",
    "# PE array setting\n",
    "# - shape of PE array X x Y\n",
    "# - whether a PE has a shared scratchpad or separate scratchpads for each datatype\n",
    "configuration_dict['PE_X'] = 14\n",
    "configuration_dict['PE_Y'] = 12\n",
    "configuration_dict['PE_SPAD_SHARED'] = False\n",
    "configuration_dict['PE_SPAD_DEPTH'] = [192, 12, 16]         # Weight, IFmap, OFmap\n",
    "configuration_dict['PE_SPAD_WIDTH'] = [16, 16, 16]\n",
    "\n",
    "# Cryptographic engine setting\n",
    "# - type of cryptographic engine + dram (LPDDR4 + AES-GCM)\n",
    "# - cycle for AES-GCM \n",
    "# - whether the cryptographic engines are shared among all datatypes or assigned to each datatype\n",
    "configuration_dict['CRYPT_ENGINE_TYPE'] = 'effective_lpddr4_aesgcm'\n",
    "configuration_dict['CRYPT_ENGINE_CYCLE_PER_BLOCK'] = 11            # avg. cycle/128bit\n",
    "\n",
    "configuration_dict['CRYPT_ENGINE_SHARED'] = False\n",
    "configuration_dict['CRYPT_ENGINE_COUNT'] = [1, 1, 1]\n",
    "\n",
    "configuration_dict['EFFECTIVE_CONSERVATIVE'] = True\n",
    "\n",
    "# Create directory for this configuration if it doesn't exist already\n",
    "# iterate through design folders to check if any pre-exisiting folder\n",
    "design_dir = 'designs/{}'.format(configuration_dict['TEMPLATE_DESIGN'])\n",
    "arch_dir = None\n",
    "total_vers = 0\n",
    "for path in os.listdir(design_dir):\n",
    "    if path != 'template' and os.path.isdir(os.path.join(design_dir, path)):\n",
    "        try:\n",
    "            with open(os.path.join(design_dir, path, 'config.yaml'), 'r') as f:\n",
    "                config_file = yaml.safe_load(f)\n",
    "            total_vers += 1\n",
    "            if config_file == configuration_dict:\n",
    "                arch_dir = path\n",
    "                print(\"Pre-existing folder found. Setting the arch_dir to {}\".format(arch_dir))\n",
    "                break\n",
    "        except:\n",
    "            print(\"No config.yaml file in the directory {}\".format(str(os.path.join(design_dir, path))))\n",
    "            \n",
    "if arch_dir is None:\n",
    "    arch_dir = 'ver{}'.format(total_vers)\n",
    "    shutil.copytree(os.path.join(design_dir, 'template'), os.path.join(design_dir, arch_dir))\n",
    "    with open(os.path.join(design_dir, arch_dir, 'config.yaml'), 'w') as f:\n",
    "        _ = yaml.dump(configuration_dict, f)\n",
    "    \n",
    "    # create baseline and effective files\n",
    "    generate_arch_files(os.path.join(design_dir, arch_dir, 'arch'), configuration_dict)\n",
    "    \n",
    "    # create scheduling / evaluation folder\n",
    "    os.mkdir(os.path.join(design_dir, arch_dir, 'scheduling'))\n",
    "    os.mkdir(os.path.join(design_dir, arch_dir, 'evaluation'))\n",
    "    \n",
    "    # create folders for baseline scheduling / evaluation\n",
    "    os.mkdir(os.path.join(design_dir, arch_dir, 'baseline_scheduling'))\n",
    "    os.mkdir(os.path.join(design_dir, arch_dir, 'baseline_evaluation'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a34a306",
   "metadata": {},
   "source": [
    "..else if you know which folder you want to use, specify here instead of running the above cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184476d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "design_dir = 'designs/{}'.format('eyeriss_like') # define your design name here\n",
    "\n",
    "arch_ver = 0\n",
    "arch_dir = 'ver{}'.format(arch_ver)              # sub directory under designs/{name}/{arch_dir}\n",
    "with open(os.path.join(design_dir, arch_dir, 'config.yaml'), 'r') as f:\n",
    "    configuration_dict = yaml.safe_load(f)\n",
    "print(\"Setting the architecture directory to: {}\".format(os.path.join(design_dir, arch_dir)))\n",
    "print(\"Printing configuration:\")\n",
    "for key, value in configuration_dict.items():\n",
    "    print(\"{}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7694a8d2",
   "metadata": {},
   "source": [
    "### Define the DNN workload\n",
    "\n",
    "Define a DNN workload in PyTorch, and convert it into a Timeloop workload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a9bf1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision.models as model_zoo\n",
    "\n",
    "import pytorch2timeloop as pytorch2timeloop\n",
    "\n",
    "# Note: this version only supports nn.Conv2d (both normal convs and depthwise/pointwise convs) and nn.Linear\n",
    "\n",
    "# AlexNet\n",
    "model_name = 'alexnet'\n",
    "net = model_zoo.alexnet(pretrained=False)\n",
    "\n",
    "# ResNet18\n",
    "# model_name = 'resnet18'\n",
    "# net = model_zoo.resnet18(pretrained=False)\n",
    "\n",
    "# MobilenetV2\n",
    "# model_name = 'mobilenet_v2'\n",
    "# net = model_zoo.mobilenet_v2(pretrained=False)\n",
    "\n",
    "# Input / Batch info\n",
    "input_size = (3, 224, 224)\n",
    "batch_size = 1\n",
    "\n",
    "print(net)\n",
    "\n",
    "# Convert to timeloop workloads; stored in workloads/{model_name}_batch{batch_size}\n",
    "top_dir = 'workloads'\n",
    "sub_dir = '{}_batch{}'.format(model_name, batch_size)\n",
    "exception_module_names = []\n",
    "\n",
    "overwrite = False\n",
    "if not os.path.exists(os.path.join(top_dir, sub_dir)) or overwrite:\n",
    "    pytorch2timeloop.convert_model(\n",
    "            net,\n",
    "            input_size,\n",
    "            batch_size,\n",
    "            sub_dir,\n",
    "            top_dir,\n",
    "            True,\n",
    "            exception_module_names\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad4e8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check duplicate layers (layer information is identical)\n",
    "# For a per-layer loopnest scheduling, only unique layers have to be searched\n",
    "\n",
    "base_dir = Path(os.getcwd())\n",
    "timeloop_dir = 'designs/{}/{}'.format(configuration_dict['TEMPLATE_DESIGN'], arch_dir)\n",
    "\n",
    "n_layers = 0\n",
    "layer_dict = {}\n",
    "layer_duplicate_info = {}\n",
    "unique_layers = []\n",
    "for module in net.modules():\n",
    "    if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n",
    "        n_layers += 1\n",
    "        if n_layers not in layer_dict.keys():\n",
    "            workload_path = os.path.join(base_dir, top_dir, sub_dir, '{}_layer{}.yaml'.format(sub_dir, n_layers))\n",
    "            with open(workload_path, 'r') as f:\n",
    "                workload_info = yaml.safe_load(f)\n",
    "            layer_dict[n_layers] = workload_info\n",
    "        \n",
    "        # identify the earliest duplicate layer\n",
    "        for key in range(1, n_layers):\n",
    "            if layer_dict[key] == layer_dict[n_layers]:\n",
    "                layer_duplicate_info[n_layers] = key\n",
    "                break\n",
    "        if n_layers not in layer_duplicate_info:\n",
    "            unique_layers.append(n_layers)\n",
    "            \n",
    "print(layer_duplicate_info)\n",
    "print(unique_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64b477d",
   "metadata": {},
   "source": [
    "### Define the top-k parameter for Timeloop\n",
    "\n",
    "Prepare the mapper.yaml for top-k search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238ca64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "topk = 6\n",
    "mapper_file_path = os.path.join(base_dir, timeloop_dir, 'mapper/mapper.yaml')\n",
    "with open(mapper_file_path, 'r') as f:\n",
    "    mapper_config = yaml.safe_load(f)\n",
    "mapper_config['mapper']['topk'] = topk\n",
    "with open(mapper_file_path, 'w') as f:\n",
    "    _ = yaml.dump(mapper_config, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549e95c3",
   "metadata": {},
   "source": [
    "### Run Timeloop for the *baseline* model (w/o considering cryptographic engines)\n",
    "\n",
    "Run timeloop for each unique layer, and convert the output to a mapping file in yaml format. Then, evaluate using the top-1 loopnest schedule for each unique layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad43aa9a",
   "metadata": {},
   "source": [
    "Running this cell can take some time depending on your model and timeloop setting.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d78f532",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_cmd(workload_info, layer_id, base_dir, timeloop_dir, sub_dir, top_dir):\n",
    "    cwd = f\"{base_dir/timeloop_dir/'baseline_scheduling'/sub_dir/f'layer{layer_id}'}\"\n",
    "    if 'M' in workload_info['problem']['instance']:\n",
    "        constraint_pth = base_dir/timeloop_dir/'constraints/*.yaml'\n",
    "    else:\n",
    "        # depthwise\n",
    "        constraint_pth = base_dir/timeloop_dir/'constraints_dw/*.yaml'\n",
    "\n",
    "    timeloopcmd = f\"timeloop-mapper-topk \" \\\n",
    "                  f\"{base_dir/timeloop_dir/'arch/baseline.yaml'} \" \\\n",
    "                  f\"{base_dir/timeloop_dir/'arch/components/*.yaml'} \" \\\n",
    "                  f\"{base_dir/timeloop_dir/'mapper/mapper.yaml'} \" \\\n",
    "                  f\"{constraint_pth} \" \\\n",
    "                  f\"{base_dir/top_dir/sub_dir/sub_dir}_layer{layer_id}.yaml \"\n",
    "    return [cwd, timeloopcmd]\n",
    "\n",
    "cwd_list = []\n",
    "cmd_list = []\n",
    "\n",
    "for layer_id in unique_layers:\n",
    "    workload_path = os.path.join(base_dir, top_dir, sub_dir, '{}_layer{}.yaml'.format(sub_dir, layer_id))\n",
    "    with open(workload_path, 'r') as f:\n",
    "        workload_info = yaml.safe_load(f)\n",
    "    [cwd, cmd] = get_cmd(workload_info, layer_id, base_dir, timeloop_dir, sub_dir, top_dir)\n",
    "    cwd_list.append(cwd)\n",
    "    cmd_list.append(cmd)\n",
    "    \n",
    "if not os.path.exists(os.path.join(base_dir, timeloop_dir, 'baseline_scheduling', sub_dir)):\n",
    "    os.mkdir(os.path.join(base_dir, timeloop_dir, 'baseline_scheduling', sub_dir))\n",
    "for cwd, cmd in zip(cwd_list, cmd_list):\n",
    "    print(\"Executing cmd: {}\".format(cmd))\n",
    "    try:\n",
    "        os.chdir(cwd)\n",
    "    except:\n",
    "        os.mkdir(cwd)\n",
    "        os.chdir(cwd)\n",
    "    os.system(cmd)\n",
    "os.chdir(base_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770b16df",
   "metadata": {},
   "source": [
    "Convert to mapping (.yaml) files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8697cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_mapping(base_dir, timeloop_dir, top_dir, sub_dir, layer_idx, topk_idx):\n",
    "    xml_file = os.path.join(base_dir, timeloop_dir, 'baseline_scheduling', sub_dir, \"layer{}\".format(layer_idx), \\\n",
    "                            \"timeloop-mapper-topk{}.map+stats.xml\".format(topk_idx))\n",
    "    workload_file = os.path.join(base_dir, top_dir, sub_dir, \"{}_layer{}.yaml\".format(sub_dir, layer_idx))\n",
    "    with open(workload_file, 'r') as f:\n",
    "        workload_info = yaml.safe_load(f)\n",
    "    if 'M' in workload_info['problem']['instance']:\n",
    "        dw = False\n",
    "    else:\n",
    "        dw = True\n",
    "    arch_constraint_file = os.path.join(base_dir, timeloop_dir, 'constraints_dw' if dw else 'constraints' , \\\n",
    "                                        'eyeriss_like_arch_constraints.yaml' if (configuration_dict['TEMPLATE_DESIGN'] == 'eyeriss_like' or \\\n",
    "                                                                                 configuration_dict['TEMPLATE_DESIGN'] == 'eyeriss_like_hbm2') \\\n",
    "                                        else 'simple_output_stationary_arch_constraints.yaml' if configuration_dict['TEMPLATE_DESIGN'] == 'output_stationary' \\\n",
    "                                        else 'simple_weight_stationary_arch_constraints.yaml')\n",
    "    mapping = xml2mapping(xml_file, workload_file, arch_constraint_file, dw)\n",
    "    with open(os.path.join(base_dir, timeloop_dir, 'baseline_scheduling',sub_dir, \"layer{}\".format(layer_idx), \\\n",
    "                           \"mapping{}.yaml\".format(topk_idx)), 'w') as f:\n",
    "        _ = yaml.dump({'mapping': mapping}, f)\n",
    "        \n",
    "for layer_idx in unique_layers:\n",
    "    for k in range(1, topk + 1):\n",
    "        convert_to_mapping(base_dir, timeloop_dir, top_dir, sub_dir, layer_idx, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c68092a",
   "metadata": {},
   "source": [
    "Evaluate the top-1 loopnest schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a3522a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_cmd_model(workload_info, layer_id, base_dir, timeloop_dir, sub_dir, top_dir):\n",
    "    cwd = f\"{base_dir/timeloop_dir/'baseline_evaluation'/sub_dir/f'layer{layer_id}'}\"\n",
    "    if 'M' in workload_info['problem']['instance']:\n",
    "        constraint_pth = base_dir/timeloop_dir/'constraints/*.yaml'\n",
    "    else:\n",
    "        # depthwise\n",
    "        constraint_pth = base_dir/timeloop_dir/'constraints_dw/*.yaml'\n",
    "\n",
    "    timeloopcmd = f\"timeloop-model \" \\\n",
    "                  f\"{base_dir/timeloop_dir/'arch/baseline.yaml'} \" \\\n",
    "                  f\"{base_dir/timeloop_dir/'arch/components/*.yaml'} \" \\\n",
    "                  f\"{base_dir/timeloop_dir/'baseline_scheduling'/sub_dir/f'layer{layer_id}/mapping1.yaml'} \" \\\n",
    "                  f\"{base_dir/top_dir/sub_dir/sub_dir}_layer{layer_id}.yaml \"\n",
    "    return [cwd, timeloopcmd]\n",
    "\n",
    "cwd_list = []\n",
    "cmd_list = []\n",
    "for layer_id in unique_layers:\n",
    "    workload_path = os.path.join(base_dir, top_dir, sub_dir, '{}_layer{}.yaml'.format(sub_dir, layer_id))\n",
    "    with open(workload_path, 'r') as f:\n",
    "        workload_info = yaml.safe_load(f)\n",
    "    [cwd, cmd] = get_cmd_model(workload_info, layer_id, base_dir, timeloop_dir, sub_dir, top_dir)\n",
    "    cwd_list.append(cwd)\n",
    "    cmd_list.append(cmd)\n",
    "    \n",
    "if not os.path.exists(os.path.join(base_dir, timeloop_dir, 'baseline_evaluation', sub_dir)):\n",
    "    os.mkdir(os.path.join(base_dir, timeloop_dir, 'baseline_evaluation', sub_dir))\n",
    "for cwd, cmd in zip(cwd_list, cmd_list):\n",
    "    print(\"Executing cmd: {}\".format(cmd))\n",
    "    try:\n",
    "        os.chdir(cwd)\n",
    "    except:\n",
    "        os.mkdir(cwd)\n",
    "        os.chdir(cwd)\n",
    "    os.system(cmd)\n",
    "os.chdir(base_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7e952b",
   "metadata": {},
   "source": [
    "### Run Timeloop for the *effective* model (considering cryptographic engines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85a0b6a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def get_cmd(workload_info, layer_id, base_dir, timeloop_dir, sub_dir, top_dir):\n",
    "    cwd = f\"{base_dir/timeloop_dir/'scheduling'/sub_dir/f'layer{layer_id}'}\"\n",
    "    if 'M' in workload_info['problem']['instance']:\n",
    "        constraint_pth = base_dir/timeloop_dir/'constraints/*.yaml'\n",
    "    else:\n",
    "        # depthwise\n",
    "        constraint_pth = base_dir/timeloop_dir/'constraints_dw/*.yaml'\n",
    "\n",
    "    timeloopcmd = f\"timeloop-mapper-topk \" \\\n",
    "                  f\"{base_dir/timeloop_dir/'arch/effective.yaml'} \" \\\n",
    "                  f\"{base_dir/timeloop_dir/'arch/components/*.yaml'} \" \\\n",
    "                  f\"{base_dir/timeloop_dir/'mapper/mapper.yaml'} \" \\\n",
    "                  f\"{constraint_pth} \" \\\n",
    "                  f\"{base_dir/top_dir/sub_dir/sub_dir}_layer{layer_id}.yaml \"\n",
    "    return [cwd, timeloopcmd]\n",
    "\n",
    "cwd_list = []\n",
    "cmd_list = []\n",
    "\n",
    "for layer_id in unique_layers:\n",
    "    workload_path = os.path.join(base_dir, top_dir, sub_dir, '{}_layer{}.yaml'.format(sub_dir, layer_id))\n",
    "    with open(workload_path, 'r') as f:\n",
    "        workload_info = yaml.safe_load(f)\n",
    "    [cwd, cmd] = get_cmd(workload_info, layer_id, base_dir, timeloop_dir, sub_dir, top_dir)\n",
    "    cwd_list.append(cwd)\n",
    "    cmd_list.append(cmd)\n",
    "    \n",
    "if not os.path.exists(os.path.join(base_dir, timeloop_dir, 'scheduling', sub_dir)):\n",
    "    os.mkdir(os.path.join(base_dir, timeloop_dir, 'scheduling', sub_dir))\n",
    "    \n",
    "start_time = time.time()\n",
    "for cwd, cmd in zip(cwd_list, cmd_list):\n",
    "    print(\"Executing cmd: {}\".format(cmd))\n",
    "    try:\n",
    "        os.chdir(cwd)\n",
    "    except:\n",
    "        os.mkdir(cwd)\n",
    "        os.chdir(cwd)\n",
    "    os.system(cmd)\n",
    "os.chdir(base_dir)\n",
    "\n",
    "# Time this cell\n",
    "print(\"Execution time: {}s\".format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d35b1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_mapping(base_dir, timeloop_dir, top_dir, sub_dir, layer_idx, topk_idx):\n",
    "    xml_file = os.path.join(base_dir, timeloop_dir, 'scheduling', sub_dir, \"layer{}\".format(layer_idx), \\\n",
    "                            \"timeloop-mapper-topk{}.map+stats.xml\".format(topk_idx))\n",
    "    workload_file = os.path.join(base_dir, top_dir, sub_dir, \"{}_layer{}.yaml\".format(sub_dir, layer_idx))\n",
    "    # print(workload_file)\n",
    "    with open(workload_file, 'r') as f:\n",
    "        workload_info = yaml.safe_load(f)\n",
    "    if 'M' in workload_info['problem']['instance']:\n",
    "        dw = False\n",
    "    else:\n",
    "        dw = True\n",
    "    arch_constraint_file = os.path.join(base_dir, timeloop_dir, 'constraints_dw' if dw else 'constraints' , \\\n",
    "                                        'eyeriss_like_arch_constraints.yaml' if (configuration_dict['TEMPLATE_DESIGN'] == 'eyeriss_like' \\\n",
    "                                                                                 or configuration_dict['TEMPLATE_DESIGN'] == 'eyeriss_like_hbm2') \\\n",
    "                                        else 'simple_output_stationary_arch_constraints.yaml' if configuration_dict['TEMPLATE_DESIGN'] == 'output_stationary' \\\n",
    "                                        else 'simple_weight_stationary_arch_constraints.yaml')\n",
    "    # print(layer_idx, dw)\n",
    "    mapping = xml2mapping(xml_file, workload_file, arch_constraint_file, dw)\n",
    "    with open(os.path.join(base_dir, timeloop_dir, 'scheduling',sub_dir, \"layer{}\".format(layer_idx), \\\n",
    "                           \"mapping{}.yaml\".format(topk_idx)), 'w') as f:\n",
    "        _ = yaml.dump({'mapping': mapping}, f)\n",
    "        \n",
    "for layer_idx in unique_layers:\n",
    "    for k in range(1, topk + 1):\n",
    "        convert_to_mapping(base_dir, timeloop_dir, top_dir, sub_dir, layer_idx, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a0735f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_cmd_model(workload_info, layer_id, base_dir, timeloop_dir, sub_dir, top_dir):\n",
    "    cwd = f\"{base_dir/timeloop_dir/'evaluation'/sub_dir/f'layer{layer_id}'}\"\n",
    "    if 'M' in workload_info['problem']['instance']:\n",
    "        constraint_pth = base_dir/timeloop_dir/'constraints/*.yaml'\n",
    "    else:\n",
    "        # depthwise\n",
    "        constraint_pth = base_dir/timeloop_dir/'constraints_dw/*.yaml'\n",
    "\n",
    "    timeloopcmd = f\"timeloop-model \" \\\n",
    "                  f\"{base_dir/timeloop_dir/'arch/baseline.yaml'} \" \\\n",
    "                  f\"{base_dir/timeloop_dir/'arch/components/*.yaml'} \" \\\n",
    "                  f\"{base_dir/timeloop_dir/'scheduling'/sub_dir/f'layer{layer_id}/mapping1.yaml'} \" \\\n",
    "                  f\"{base_dir/top_dir/sub_dir/sub_dir}_layer{layer_id}.yaml \"\n",
    "    return [cwd, timeloopcmd]\n",
    "\n",
    "cwd_list = []\n",
    "cmd_list = []\n",
    "for layer_id in unique_layers:\n",
    "    workload_path = os.path.join(base_dir, top_dir, sub_dir, '{}_layer{}.yaml'.format(sub_dir, layer_id))\n",
    "    with open(workload_path, 'r') as f:\n",
    "        workload_info = yaml.safe_load(f)\n",
    "    [cwd, cmd] = get_cmd_model(workload_info, layer_id, base_dir, timeloop_dir, sub_dir, top_dir)\n",
    "    cwd_list.append(cwd)\n",
    "    cmd_list.append(cmd)\n",
    "    \n",
    "if not os.path.exists(os.path.join(base_dir, timeloop_dir, 'evaluation', sub_dir)):\n",
    "    os.mkdir(os.path.join(base_dir, timeloop_dir, 'evaluation', sub_dir))\n",
    "for cwd, cmd in zip(cwd_list, cmd_list):\n",
    "    print(\"Executing cmd: {}\".format(cmd))\n",
    "    try:\n",
    "        os.chdir(cwd)\n",
    "    except:\n",
    "        os.mkdir(cwd)\n",
    "        os.chdir(cwd)\n",
    "    os.system(cmd)\n",
    "os.chdir(base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f04ff41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
